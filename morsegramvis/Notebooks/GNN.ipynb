{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# creating dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchviz in /home/rathoddinesh/.local/lib/python3.8/site-packages (0.0.2)\n",
      "Requirement already satisfied: torch in /home/rathoddinesh/.local/lib/python3.8/site-packages (from torchviz) (2.0.0)\n",
      "Requirement already satisfied: graphviz in /home/rathoddinesh/.local/lib/python3.8/site-packages (from torchviz) (0.20.1)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/rathoddinesh/.local/lib/python3.8/site-packages (from torch->torchviz) (11.4.0.1)\n",
      "Requirement already satisfied: networkx in /home/rathoddinesh/.local/lib/python3.8/site-packages (from torch->torchviz) (2.8.4)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/rathoddinesh/.local/lib/python3.8/site-packages (from torch->torchviz) (11.7.101)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/rathoddinesh/.local/lib/python3.8/site-packages (from torch->torchviz) (10.2.10.91)\n",
      "Requirement already satisfied: jinja2 in /home/rathoddinesh/.local/lib/python3.8/site-packages (from torch->torchviz) (3.1.2)\n",
      "Requirement already satisfied: triton==2.0.0; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/rathoddinesh/.local/lib/python3.8/site-packages (from torch->torchviz) (2.0.0)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/rathoddinesh/.local/lib/python3.8/site-packages (from torch->torchviz) (10.9.0.58)\n",
      "Requirement already satisfied: sympy in /home/rathoddinesh/.local/lib/python3.8/site-packages (from torch->torchviz) (1.11.1)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/rathoddinesh/.local/lib/python3.8/site-packages (from torch->torchviz) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/rathoddinesh/.local/lib/python3.8/site-packages (from torch->torchviz) (11.7.4.91)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/rathoddinesh/.local/lib/python3.8/site-packages (from torch->torchviz) (11.7.99)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/rathoddinesh/.local/lib/python3.8/site-packages (from torch->torchviz) (11.7.91)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/rathoddinesh/.local/lib/python3.8/site-packages (from torch->torchviz) (11.7.99)\n",
      "Requirement already satisfied: typing-extensions in /home/rathoddinesh/.local/lib/python3.8/site-packages (from torch->torchviz) (4.3.0)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/rathoddinesh/.local/lib/python3.8/site-packages (from torch->torchviz) (8.5.0.96)\n",
      "Requirement already satisfied: filelock in /home/rathoddinesh/.local/lib/python3.8/site-packages (from torch->torchviz) (3.11.0)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/rathoddinesh/.local/lib/python3.8/site-packages (from torch->torchviz) (2.14.3)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from nvidia-cuda-cupti-cu11==11.7.101; platform_system == \"Linux\" and platform_machine == \"x86_64\"->torch->torchviz) (45.2.0)\n",
      "Requirement already satisfied: wheel in /usr/lib/python3/dist-packages (from nvidia-cuda-cupti-cu11==11.7.101; platform_system == \"Linux\" and platform_machine == \"x86_64\"->torch->torchviz) (0.34.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/rathoddinesh/.local/lib/python3.8/site-packages (from jinja2->torch->torchviz) (2.1.1)\n",
      "Requirement already satisfied: lit in /home/rathoddinesh/.local/lib/python3.8/site-packages (from triton==2.0.0; platform_system == \"Linux\" and platform_machine == \"x86_64\"->torch->torchviz) (16.0.1)\n",
      "Requirement already satisfied: cmake in /home/rathoddinesh/.local/lib/python3.8/site-packages (from triton==2.0.0; platform_system == \"Linux\" and platform_machine == \"x86_64\"->torch->torchviz) (3.26.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/rathoddinesh/.local/lib/python3.8/site-packages (from sympy->torch->torchviz) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torchviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import Dataset, Data\n",
    "from tqdm import tqdm\n",
    "from torchviz import make_dot\n",
    "import torch\n",
    "import os\n",
    "import vtk\n",
    "import numpy as np\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# custom logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pandas as pd\n",
    "\n",
    "class MLLogger:\n",
    "    def __init__(self, model_name):\n",
    "        self.model_name = model_name\n",
    "        self.log = pd.DataFrame(columns=['timestamp', 'epoch', 'train_loss', 'train_accuracy', 'test_loss', 'test_accuracy'])\n",
    "\n",
    "    def log_metrics(self, epoch, train_loss, train_accuracy, test_loss, test_accuracy):\n",
    "        now = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "        row = { 'timestamp': now,\n",
    "                                'epoch': epoch, \n",
    "                                'train_loss': train_loss, \n",
    "                                'train_accuracy': train_accuracy, \n",
    "                                'test_loss': test_loss, \n",
    "                                'test_accuracy': test_accuracy}\n",
    "        # self.log = self.log.append(row, ignore_index=True)\n",
    "        # use concat instead of append to avoid copying the whole dataframe\n",
    "        self.log = pd.concat([self.log, pd.DataFrame([row])], ignore_index=True)\n",
    "\n",
    "    def save_log(self, file_path):\n",
    "        logs_folder = \"../.morsegram/logs\"\n",
    "        if not os.path.exists(logs_folder):\n",
    "            os.makedirs(logs_folder)\n",
    "        self.log.to_csv(os.path.join(logs_folder, self.model_name + \"_\" + file_path), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Triples():\n",
    "    def __init__(self):\n",
    "        self.cp_id1 = 0\n",
    "        self.cp_id2 = 0\n",
    "        self.cp_id3 = 0\n",
    "        self.pos1 = (0, 0, 0)\n",
    "        self.pos2 = (0, 0, 0)\n",
    "        self.pos3 = (0, 0, 0)\n",
    "        self.val1 = 0\n",
    "        self.val2 = 0\n",
    "        self.val3 = 0\n",
    "        self.alpha = 0\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset for Morsegram Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MorseGramDataset(Dataset):\n",
    "    def __init__(self, root, transform=None, pre_transform=None):\n",
    "        self.files_metadata_dict = {}\n",
    "        super(MorseGramDataset, self).__init__(root, transform, pre_transform)\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return []\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        list_data_folders = []\n",
    "        # adding folder names\n",
    "        for file in os.listdir(self.root + \"/raw/\"):\n",
    "            if os.path.isdir(self.root + \"/raw/\" + file):\n",
    "                list_data_folders.append(file)\n",
    "        output_files = []\n",
    "        for folder in list_data_folders:\n",
    "            for label in ['0', '1']:\n",
    "                for file in os.listdir(self.root + \"/raw/\" + folder + \"/simplified/\" + label + \"/\"):\n",
    "                    if file.endswith(\".vtp\"):\n",
    "                        output_files.append(f'data_{len(output_files)}.pt')\n",
    "\n",
    "        return output_files\n",
    "\n",
    "    def download(self):\n",
    "        pass\n",
    "\n",
    "    def process(self):\n",
    "        list_data_folders = []\n",
    "        start_file_no = 0\n",
    "        # adding folder names\n",
    "        for file in os.listdir(self.root + \"/raw/\"):\n",
    "            if os.path.isdir(self.root + \"/raw/\" + file):\n",
    "                list_data_folders.append(file)\n",
    "\n",
    "        for folder in tqdm(list_data_folders):\n",
    "            start_file_no += self._process_single_data_folder(\n",
    "                folder, start_file_no)\n",
    "\n",
    "        # save metadata file\n",
    "        with open(self.processed_dir + \"/metadata.txt\", \"w\") as f:\n",
    "            f.write(str(self.files_metadata_dict))\n",
    "\n",
    "    def _process_single_data_folder(self, folder_name, start_file_no):\n",
    "\n",
    "        reader = vtk.vtkXMLPolyDataReader()\n",
    "        reader.SetFileName(self.root + \"/raw/\" + folder_name + \"/cps_3.vtp\")\n",
    "        reader.Update()\n",
    "        self.cp3_data = reader.GetOutput()\n",
    "\n",
    "        file_count = 0\n",
    "        for label in ['0', '1']:\n",
    "            for file in os.listdir(self.root + \"/raw/\" + folder_name + \"/simplified/\" + label + \"/\"):\n",
    "                reader = vtk.vtkXMLPolyDataReader()\n",
    "                reader.SetFileName(\n",
    "                    self.root + \"/raw/\" + folder_name + \"/simplified/\" + label + \"/\" + file)\n",
    "                reader.Update()\n",
    "                data = reader.GetOutput()\n",
    "\n",
    "                max1_ids = data.GetPointData().GetArray(\"Max1 ID\")\n",
    "                max2_ids = data.GetPointData().GetArray(\"Max2 ID\")\n",
    "                vals = data.GetPointData().GetArray(\"CP Value\")\n",
    "                sad_ids = data.GetPointData().GetArray(\"CP ID\")\n",
    "                max1_value = data.GetPointData().GetArray(\"Max1 Value\")\n",
    "                max2_value = data.GetPointData().GetArray(\"Max2 Value\")\n",
    "\n",
    "                cp3_ids = self.cp3_data.GetPointData().GetArray(\"CP ID\")\n",
    "                cp3_id_pos_dict = {}\n",
    "                for i in range(self.cp3_data.GetNumberOfPoints()):\n",
    "                    cp3_id_pos_dict[cp3_ids.GetValue(\n",
    "                        i)] = self.cp3_data.GetPoint(i)\n",
    "\n",
    "                numPoints = data.GetNumberOfPoints()\n",
    "                list_triples = []\n",
    "                all_nodes = set()\n",
    "                # adding the position of saddle points\n",
    "                for i in range(numPoints):\n",
    "                    triple = Triples()\n",
    "                    triple.cp_id1 = max1_ids.GetValue(i)\n",
    "                    triple.pos1 = cp3_id_pos_dict[max1_ids.GetValue(i)]\n",
    "                    triple.val1 = max1_value.GetValue(i)\n",
    "                    all_nodes.add((triple.cp_id1, 3, triple.val1, 0))\n",
    "\n",
    "                    triple.cp_id2 = sad_ids.GetValue(i)\n",
    "                    triple.pos2 = data.GetPoint(i)\n",
    "                    triple.val2 = vals.GetValue(i)\n",
    "                    all_nodes.add((triple.cp_id2, 2, triple.val2, triple.alpha))\n",
    "\n",
    "                    triple.cp_id3 = max2_ids.GetValue(i)\n",
    "                    triple.pos3 = cp3_id_pos_dict[max2_ids.GetValue(i)]\n",
    "                    triple.val3 = max2_value.GetValue(i)\n",
    "                    all_nodes.add((triple.cp_id3, 3, triple.val3, 0))\n",
    "\n",
    "                    if triple.val1 > triple.val3:\n",
    "                        triple.alpha = 1 - (triple.val2 / triple.val3)\n",
    "                    else:\n",
    "                        triple.alpha = 1 - (triple.val2 / triple.val1)\n",
    "                    list_triples.append(triple)\n",
    "\n",
    "                self.nodes_new_id_dict = {}\n",
    "                for i, node_data in enumerate(all_nodes):\n",
    "                    self.nodes_new_id_dict[node_data[0]] = i\n",
    "\n",
    "                node_feaures = self._get_node_features(all_nodes)\n",
    "                edge_features = self._get_edge_features(list_triples)\n",
    "                edge_index = self._get_adj_info(list_triples)\n",
    "                g_label = self._get_label(int(label))\n",
    "\n",
    "                data = Data(x=node_feaures,\n",
    "                            edge_index=edge_index,\n",
    "                            edge_attr=edge_features,\n",
    "                            y=g_label,\n",
    "                            dataset_name=folder_name,\n",
    "                            file_name=file)\n",
    "\n",
    "                torch.save(data, os.path.join(self.processed_dir,\n",
    "                           f'data_{start_file_no + file_count}.pt'))\n",
    "                self.files_metadata_dict[start_file_no + file_count] = {\n",
    "                    'folder_name': folder_name, 'file_name': file, 'label': label}\n",
    "                file_count += 1\n",
    "\n",
    "        return file_count\n",
    "\n",
    "    def _get_node_features(self, data):\n",
    "\n",
    "        num_nodes = len(self.nodes_new_id_dict)\n",
    "        all_nodes_features = [[] for i in range(num_nodes)]\n",
    "\n",
    "        for i, node_data in enumerate(data):\n",
    "            n_cp_id, n_type, n_value, n_apha = node_data\n",
    "            n_new_id = self.nodes_new_id_dict[n_cp_id]\n",
    "            all_nodes_features[n_new_id].append(n_type)\n",
    "            all_nodes_features[n_new_id].append(n_value)\n",
    "            all_nodes_features[n_new_id].append(n_apha)\n",
    "\n",
    "        all_nodes_features = np.asarray(all_nodes_features)\n",
    "        return torch.tensor(all_nodes_features, dtype=torch.float)\n",
    "\n",
    "    def _get_edge_features(self, data):\n",
    "        all_edge_features = []\n",
    "        for i, triples in enumerate(data):\n",
    "            edge1_a = [*triples.pos1]\n",
    "            edge1_b = [*triples.pos2]\n",
    "            edge2_a = [*triples.pos2]\n",
    "            edge2_b = [*triples.pos3]\n",
    "            all_edge_features.append(\n",
    "                np.sqrt(np.sum(np.square(np.subtract(edge1_a, edge1_b)))))\n",
    "            all_edge_features.append(\n",
    "                np.sqrt(np.sum(np.square(np.subtract(edge2_a, edge2_b)))))\n",
    "\n",
    "        all_edge_features = np.asarray(all_edge_features)\n",
    "        return torch.tensor(all_edge_features, dtype=torch.float)\n",
    "\n",
    "    def _get_adj_info(self, data):\n",
    "\n",
    "        # 2 x num_edges\n",
    "        all_edges = np.zeros((2, 2*len(data)))\n",
    "\n",
    "        for i, triples in enumerate(data):\n",
    "            n1_id = self.nodes_new_id_dict[triples.cp_id1]\n",
    "            n2_id = self.nodes_new_id_dict[triples.cp_id2]\n",
    "            n3_id = self.nodes_new_id_dict[triples.cp_id3]\n",
    "            all_edges[0][2*i] = n1_id\n",
    "            all_edges[1][2*i] = n2_id\n",
    "            all_edges[0][2*i+1] = n2_id\n",
    "            all_edges[1][2*i+1] = n3_id\n",
    "\n",
    "        return torch.tensor(all_edges, dtype=torch.long)\n",
    "\n",
    "    def _get_label(self, data):\n",
    "        label = np.asarray([data])\n",
    "        return torch.tensor(label, dtype=torch.int64)\n",
    "\n",
    "    def len(self):\n",
    "        return len(self.processed_file_names)\n",
    "\n",
    "    def get(self, idx):\n",
    "        data = torch.load(os.path.join(self.processed_dir, f'data_{idx}.pt'))\n",
    "        return data\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [],
   "source": [
    "train_dataset = MorseGramDataset(root='data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1690"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[84, 3], edge_index=[2, 96], edge_attr=[96], y=[1], dataset_name='odo', file_name='276574.vtp')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training graphs: 1352\n",
      "Number of test graphs: 338\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(12345)\n",
    "dataset = train_dataset.shuffle()\n",
    "num_data = len(dataset)\n",
    "train_dataset = dataset[:int(num_data*0.8)]\n",
    "test_dataset = dataset[int(num_data*0.8):]\n",
    "\n",
    "print(f'Number of training graphs: {len(train_dataset)}')\n",
    "print(f'Number of test graphs: {len(test_dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of graphs with label 0 in train dataset: 193\n",
      "Number of graphs with label 1 in train dataset: 1159\n",
      "Number of graphs with label 0 in test dataset: 60\n",
      "Number of graphs with label 1 in test dataset: 278\n"
     ]
    }
   ],
   "source": [
    "# different labels in test and train dataset\n",
    "num_0 = 0\n",
    "num_1 = 0\n",
    "for i in range(len(train_dataset)):\n",
    "    if train_dataset[i].y == 0:\n",
    "        num_0 += 1\n",
    "    else:\n",
    "        num_1 += 1\n",
    "print(f'Number of graphs with label 0 in train dataset: {num_0}')\n",
    "print(f'Number of graphs with label 1 in train dataset: {num_1}')\n",
    "\n",
    "num_0 = 0\n",
    "num_1 = 0\n",
    "for i in range(len(test_dataset)):\n",
    "    if test_dataset[i].y == 0:\n",
    "        num_0 += 1\n",
    "    else:\n",
    "        num_1 += 1\n",
    "print(f'Number of graphs with label 0 in test dataset: {num_0}')\n",
    "print(f'Number of graphs with label 1 in test dataset: {num_1}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1:\n",
      "=======\n",
      "Number of graphs in the current batch: 64\n",
      "DataBatch(x=[7080, 3], edge_index=[2, 8284], edge_attr=[8284], y=[64], dataset_name=[64], file_name=[64], batch=[7080], ptr=[65])\n",
      "\n",
      "Step 2:\n",
      "=======\n",
      "Number of graphs in the current batch: 64\n",
      "DataBatch(x=[5447, 3], edge_index=[2, 6298], edge_attr=[6298], y=[64], dataset_name=[64], file_name=[64], batch=[5447], ptr=[65])\n",
      "\n",
      "Step 3:\n",
      "=======\n",
      "Number of graphs in the current batch: 64\n",
      "DataBatch(x=[6189, 3], edge_index=[2, 7194], edge_attr=[7194], y=[64], dataset_name=[64], file_name=[64], batch=[6189], ptr=[65])\n",
      "\n",
      "Step 4:\n",
      "=======\n",
      "Number of graphs in the current batch: 64\n",
      "DataBatch(x=[6209, 3], edge_index=[2, 7266], edge_attr=[7266], y=[64], dataset_name=[64], file_name=[64], batch=[6209], ptr=[65])\n",
      "\n",
      "Step 5:\n",
      "=======\n",
      "Number of graphs in the current batch: 64\n",
      "DataBatch(x=[5968, 3], edge_index=[2, 6916], edge_attr=[6916], y=[64], dataset_name=[64], file_name=[64], batch=[5968], ptr=[65])\n",
      "\n",
      "Step 6:\n",
      "=======\n",
      "Number of graphs in the current batch: 64\n",
      "DataBatch(x=[5718, 3], edge_index=[2, 6610], edge_attr=[6610], y=[64], dataset_name=[64], file_name=[64], batch=[5718], ptr=[65])\n",
      "\n",
      "Step 7:\n",
      "=======\n",
      "Number of graphs in the current batch: 64\n",
      "DataBatch(x=[5033, 3], edge_index=[2, 5812], edge_attr=[5812], y=[64], dataset_name=[64], file_name=[64], batch=[5033], ptr=[65])\n",
      "\n",
      "Step 8:\n",
      "=======\n",
      "Number of graphs in the current batch: 64\n",
      "DataBatch(x=[6060, 3], edge_index=[2, 7084], edge_attr=[7084], y=[64], dataset_name=[64], file_name=[64], batch=[6060], ptr=[65])\n",
      "\n",
      "Step 9:\n",
      "=======\n",
      "Number of graphs in the current batch: 64\n",
      "DataBatch(x=[5889, 3], edge_index=[2, 6824], edge_attr=[6824], y=[64], dataset_name=[64], file_name=[64], batch=[5889], ptr=[65])\n",
      "\n",
      "Step 10:\n",
      "=======\n",
      "Number of graphs in the current batch: 64\n",
      "DataBatch(x=[6056, 3], edge_index=[2, 7116], edge_attr=[7116], y=[64], dataset_name=[64], file_name=[64], batch=[6056], ptr=[65])\n",
      "\n",
      "Step 11:\n",
      "=======\n",
      "Number of graphs in the current batch: 64\n",
      "DataBatch(x=[5082, 3], edge_index=[2, 5870], edge_attr=[5870], y=[64], dataset_name=[64], file_name=[64], batch=[5082], ptr=[65])\n",
      "\n",
      "Step 12:\n",
      "=======\n",
      "Number of graphs in the current batch: 64\n",
      "DataBatch(x=[5501, 3], edge_index=[2, 6402], edge_attr=[6402], y=[64], dataset_name=[64], file_name=[64], batch=[5501], ptr=[65])\n",
      "\n",
      "Step 13:\n",
      "=======\n",
      "Number of graphs in the current batch: 64\n",
      "DataBatch(x=[6370, 3], edge_index=[2, 7468], edge_attr=[7468], y=[64], dataset_name=[64], file_name=[64], batch=[6370], ptr=[65])\n",
      "\n",
      "Step 14:\n",
      "=======\n",
      "Number of graphs in the current batch: 64\n",
      "DataBatch(x=[6426, 3], edge_index=[2, 7504], edge_attr=[7504], y=[64], dataset_name=[64], file_name=[64], batch=[6426], ptr=[65])\n",
      "\n",
      "Step 15:\n",
      "=======\n",
      "Number of graphs in the current batch: 64\n",
      "DataBatch(x=[5060, 3], edge_index=[2, 5806], edge_attr=[5806], y=[64], dataset_name=[64], file_name=[64], batch=[5060], ptr=[65])\n",
      "\n",
      "Step 16:\n",
      "=======\n",
      "Number of graphs in the current batch: 64\n",
      "DataBatch(x=[5337, 3], edge_index=[2, 6208], edge_attr=[6208], y=[64], dataset_name=[64], file_name=[64], batch=[5337], ptr=[65])\n",
      "\n",
      "Step 17:\n",
      "=======\n",
      "Number of graphs in the current batch: 64\n",
      "DataBatch(x=[5325, 3], edge_index=[2, 6164], edge_attr=[6164], y=[64], dataset_name=[64], file_name=[64], batch=[5325], ptr=[65])\n",
      "\n",
      "Step 18:\n",
      "=======\n",
      "Number of graphs in the current batch: 64\n",
      "DataBatch(x=[6264, 3], edge_index=[2, 7298], edge_attr=[7298], y=[64], dataset_name=[64], file_name=[64], batch=[6264], ptr=[65])\n",
      "\n",
      "Step 19:\n",
      "=======\n",
      "Number of graphs in the current batch: 64\n",
      "DataBatch(x=[5881, 3], edge_index=[2, 6838], edge_attr=[6838], y=[64], dataset_name=[64], file_name=[64], batch=[5881], ptr=[65])\n",
      "\n",
      "Step 20:\n",
      "=======\n",
      "Number of graphs in the current batch: 64\n",
      "DataBatch(x=[5959, 3], edge_index=[2, 6990], edge_attr=[6990], y=[64], dataset_name=[64], file_name=[64], batch=[5959], ptr=[65])\n",
      "\n",
      "Step 21:\n",
      "=======\n",
      "Number of graphs in the current batch: 64\n",
      "DataBatch(x=[6035, 3], edge_index=[2, 7010], edge_attr=[7010], y=[64], dataset_name=[64], file_name=[64], batch=[6035], ptr=[65])\n",
      "\n",
      "Step 22:\n",
      "=======\n",
      "Number of graphs in the current batch: 8\n",
      "DataBatch(x=[1186, 3], edge_index=[2, 1400], edge_attr=[1400], y=[8], dataset_name=[8], file_name=[8], batch=[1186], ptr=[9])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "for step, data in enumerate(train_loader):\n",
    "    print(f'Step {step + 1}:')\n",
    "    print('=======')\n",
    "    print(f'Number of graphs in the current batch: {data.num_graphs}')\n",
    "    print(data)\n",
    "    print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCN(\n",
      "  (conv1): GCNConv(3, 64)\n",
      "  (conv2): GCNConv(64, 64)\n",
      "  (conv3): GCNConv(64, 64)\n",
      "  (lin): Linear(in_features=64, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import global_mean_pool, global_max_pool\n",
    "\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super(GCN, self).__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        self.conv1 = GCNConv(dataset.num_node_features, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.conv3 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.lin = Linear(hidden_channels, dataset.num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        # 1. Obtain node embeddings \n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv3(x, edge_index)\n",
    "\n",
    "        # 2. Readout layer\n",
    "        # x = global_mean_pool(x, batch)  # [batch_size, hidden_channels]\n",
    "        x = global_max_pool(x, batch)\n",
    "\n",
    "        # 3. Apply a final classifier\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.lin(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "model = GCN(hidden_channels=64)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "google.colab.output.setIframeHeight(0, true, {maxHeight: 300})",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 002, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 003, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 004, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 005, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 006, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 007, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 008, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 009, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 010, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 011, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 012, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 013, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 014, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 015, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 016, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 017, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 018, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 019, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 020, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 021, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 022, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 023, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 024, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 025, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 026, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 027, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 028, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 029, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 030, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 031, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 032, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 033, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 034, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 035, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 036, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 037, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 038, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 039, Train Acc: 0.8558, Test Acc: 0.8225\n",
      "Epoch: 040, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 041, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 042, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 043, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 044, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 045, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 046, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 047, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 048, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 049, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 050, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 051, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 052, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 053, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 054, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 055, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 056, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 057, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 058, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 059, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 060, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 061, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 062, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 063, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 064, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 065, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 066, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 067, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 068, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 069, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 070, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 071, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 072, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 073, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 074, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 075, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 076, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 077, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 078, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 079, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 080, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 081, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 082, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 083, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 084, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 085, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 086, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 087, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 088, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 089, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 090, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 091, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 092, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 093, Train Acc: 0.8565, Test Acc: 0.8225\n",
      "Epoch: 094, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 095, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 096, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 097, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 098, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 099, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 100, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 101, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 102, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 103, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 104, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 105, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 106, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 107, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 108, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 109, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 110, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 111, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 112, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 113, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 114, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 115, Train Acc: 0.8565, Test Acc: 0.8225\n",
      "Epoch: 116, Train Acc: 0.8565, Test Acc: 0.8225\n",
      "Epoch: 117, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 118, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 119, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 120, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 121, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 122, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 123, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 124, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 125, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 126, Train Acc: 0.8572, Test Acc: 0.8195\n",
      "Epoch: 127, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 128, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 129, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 130, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 131, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 132, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 133, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 134, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 135, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 136, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 137, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 138, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 139, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 140, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 141, Train Acc: 0.8580, Test Acc: 0.8195\n",
      "Epoch: 142, Train Acc: 0.8565, Test Acc: 0.8225\n",
      "Epoch: 143, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 144, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 145, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 146, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 147, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 148, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 149, Train Acc: 0.8565, Test Acc: 0.8225\n",
      "Epoch: 150, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 151, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 152, Train Acc: 0.8587, Test Acc: 0.8195\n",
      "Epoch: 153, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 154, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 155, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 156, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 157, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 158, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 159, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 160, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 161, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 162, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 163, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 164, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 165, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 166, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 167, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 168, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 169, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 170, Train Acc: 0.8572, Test Acc: 0.8225\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import Javascript\n",
    "display(Javascript('''google.colab.output.setIframeHeight(0, true, {maxHeight: 300})'''))\n",
    "\n",
    "model = GCN(hidden_channels=64)\n",
    "logger1 = MLLogger('gcn')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "\n",
    "    for data in train_loader:  # Iterate in batches over the training dataset.\n",
    "         out = model(data.x, data.edge_index, data.batch)  # Perform a single forward pass.\n",
    "         loss = criterion(out, data.y)  # Compute the loss.\n",
    "         loss.backward()  # Derive gradients.\n",
    "         optimizer.step()  # Update parameters based on gradients.\n",
    "         optimizer.zero_grad()  # Clear gradients.\n",
    "\n",
    "def test(loader):\n",
    "     model.eval()\n",
    "\n",
    "     correct = 0\n",
    "     for data in loader:  # Iterate in batches over the training/test dataset.\n",
    "         out = model(data.x, data.edge_index, data.batch)  \n",
    "         pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "         correct += int((pred == data.y).sum())  # Check against ground-truth labels.\n",
    "     return correct / len(loader.dataset)  # Derive ratio of correct predictions.\n",
    "\n",
    "\n",
    "for epoch in range(1, 171):\n",
    "    train()\n",
    "    train_acc = test(train_loader)\n",
    "    test_acc = test(test_loader)\n",
    "    print(f'Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}')\n",
    "    logger1.log_metrics(epoch=epoch,\n",
    "                        test_accuracy=test_acc,\n",
    "                        train_accuracy=train_acc,\n",
    "                        test_loss=None,\n",
    "                        train_loss=None)\n",
    "    \n",
    "\n",
    "# save logs\n",
    "log_name = str(datetime.datetime.now()).replace(\" \", \"_\").replace(\":\", \"_\").replace(\".\", \"_\")\n",
    "logger1.save_log(log_name + \".csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sample output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pred = model(test_dataset[0].x, test_dataset[0].edge_index, test_dataset[0].batch)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GraphConv Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GNN(\n",
      "  (conv1): GraphConv(3, 64)\n",
      "  (conv2): GraphConv(64, 64)\n",
      "  (conv3): GraphConv(64, 64)\n",
      "  (lin): Linear(in_features=64, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.nn import GraphConv\n",
    "\n",
    "\n",
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super(GNN, self).__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        self.conv1 = GraphConv(dataset.num_node_features, hidden_channels)\n",
    "        self.conv2 = GraphConv(hidden_channels, hidden_channels)\n",
    "        self.conv3 = GraphConv(hidden_channels, hidden_channels)\n",
    "        self.lin = Linear(hidden_channels, dataset.num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv3(x, edge_index)\n",
    "\n",
    "        # x = global_mean_pool(x, batch)\n",
    "        x = global_max_pool(x, batch)\n",
    "\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.lin(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "model = GNN(hidden_channels=64)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "google.colab.output.setIframeHeight(0, true, {maxHeight: 300})",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GNN(\n",
      "  (conv1): GraphConv(3, 64)\n",
      "  (conv2): GraphConv(64, 64)\n",
      "  (conv3): GraphConv(64, 64)\n",
      "  (lin): Linear(in_features=64, out_features=2, bias=True)\n",
      ")\n",
      "Epoch: 001, Train Acc: 0.8565, Test Acc: 0.8225\n",
      "Epoch: 002, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 003, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 004, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 005, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 006, Train Acc: 0.8565, Test Acc: 0.8225\n",
      "Epoch: 007, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 008, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 009, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 010, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 011, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 012, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 013, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 014, Train Acc: 0.8565, Test Acc: 0.8225\n",
      "Epoch: 015, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 016, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 017, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 018, Train Acc: 0.8565, Test Acc: 0.8225\n",
      "Epoch: 019, Train Acc: 0.8543, Test Acc: 0.8225\n",
      "Epoch: 020, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 021, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 022, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 023, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 024, Train Acc: 0.8543, Test Acc: 0.8225\n",
      "Epoch: 025, Train Acc: 0.8558, Test Acc: 0.8225\n",
      "Epoch: 026, Train Acc: 0.8550, Test Acc: 0.8225\n",
      "Epoch: 027, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 028, Train Acc: 0.8580, Test Acc: 0.8225\n",
      "Epoch: 029, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 030, Train Acc: 0.8580, Test Acc: 0.8225\n",
      "Epoch: 031, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 032, Train Acc: 0.8617, Test Acc: 0.8225\n",
      "Epoch: 033, Train Acc: 0.8565, Test Acc: 0.8225\n",
      "Epoch: 034, Train Acc: 0.8587, Test Acc: 0.8225\n",
      "Epoch: 035, Train Acc: 0.8580, Test Acc: 0.8195\n",
      "Epoch: 036, Train Acc: 0.8580, Test Acc: 0.8195\n",
      "Epoch: 037, Train Acc: 0.8595, Test Acc: 0.8225\n",
      "Epoch: 038, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 039, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 040, Train Acc: 0.8580, Test Acc: 0.8225\n",
      "Epoch: 041, Train Acc: 0.8617, Test Acc: 0.8195\n",
      "Epoch: 042, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 043, Train Acc: 0.8587, Test Acc: 0.8225\n",
      "Epoch: 044, Train Acc: 0.8580, Test Acc: 0.8225\n",
      "Epoch: 045, Train Acc: 0.8580, Test Acc: 0.8284\n",
      "Epoch: 046, Train Acc: 0.8558, Test Acc: 0.8225\n",
      "Epoch: 047, Train Acc: 0.8580, Test Acc: 0.8402\n",
      "Epoch: 048, Train Acc: 0.8639, Test Acc: 0.8284\n",
      "Epoch: 049, Train Acc: 0.8595, Test Acc: 0.8225\n",
      "Epoch: 050, Train Acc: 0.8558, Test Acc: 0.8166\n",
      "Epoch: 051, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 052, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 053, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 054, Train Acc: 0.8595, Test Acc: 0.8225\n",
      "Epoch: 055, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 056, Train Acc: 0.8609, Test Acc: 0.8225\n",
      "Epoch: 057, Train Acc: 0.8587, Test Acc: 0.8225\n",
      "Epoch: 058, Train Acc: 0.8580, Test Acc: 0.8225\n",
      "Epoch: 059, Train Acc: 0.8558, Test Acc: 0.8166\n",
      "Epoch: 060, Train Acc: 0.8580, Test Acc: 0.8225\n",
      "Epoch: 061, Train Acc: 0.8595, Test Acc: 0.8225\n",
      "Epoch: 062, Train Acc: 0.8602, Test Acc: 0.8225\n",
      "Epoch: 063, Train Acc: 0.8587, Test Acc: 0.8225\n",
      "Epoch: 064, Train Acc: 0.8580, Test Acc: 0.8225\n",
      "Epoch: 065, Train Acc: 0.8624, Test Acc: 0.8225\n",
      "Epoch: 066, Train Acc: 0.8572, Test Acc: 0.8254\n",
      "Epoch: 067, Train Acc: 0.8521, Test Acc: 0.8136\n",
      "Epoch: 068, Train Acc: 0.8587, Test Acc: 0.8254\n",
      "Epoch: 069, Train Acc: 0.8617, Test Acc: 0.8225\n",
      "Epoch: 070, Train Acc: 0.8587, Test Acc: 0.8225\n",
      "Epoch: 071, Train Acc: 0.8595, Test Acc: 0.8225\n",
      "Epoch: 072, Train Acc: 0.8580, Test Acc: 0.8225\n",
      "Epoch: 073, Train Acc: 0.8624, Test Acc: 0.8254\n",
      "Epoch: 074, Train Acc: 0.8602, Test Acc: 0.8225\n",
      "Epoch: 075, Train Acc: 0.8617, Test Acc: 0.8225\n",
      "Epoch: 076, Train Acc: 0.8587, Test Acc: 0.8225\n",
      "Epoch: 077, Train Acc: 0.8632, Test Acc: 0.8195\n",
      "Epoch: 078, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 079, Train Acc: 0.8558, Test Acc: 0.8136\n",
      "Epoch: 080, Train Acc: 0.8617, Test Acc: 0.8225\n",
      "Epoch: 081, Train Acc: 0.8609, Test Acc: 0.8254\n",
      "Epoch: 082, Train Acc: 0.8669, Test Acc: 0.8254\n",
      "Epoch: 083, Train Acc: 0.8602, Test Acc: 0.8195\n",
      "Epoch: 084, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 085, Train Acc: 0.8565, Test Acc: 0.8136\n",
      "Epoch: 086, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 087, Train Acc: 0.8595, Test Acc: 0.8225\n",
      "Epoch: 088, Train Acc: 0.8587, Test Acc: 0.8225\n",
      "Epoch: 089, Train Acc: 0.8572, Test Acc: 0.8225\n",
      "Epoch: 090, Train Acc: 0.8580, Test Acc: 0.8225\n",
      "Epoch: 091, Train Acc: 0.8595, Test Acc: 0.8225\n",
      "Epoch: 092, Train Acc: 0.8617, Test Acc: 0.8107\n",
      "Epoch: 093, Train Acc: 0.8587, Test Acc: 0.8225\n",
      "Epoch: 094, Train Acc: 0.8617, Test Acc: 0.8225\n",
      "Epoch: 095, Train Acc: 0.8580, Test Acc: 0.8225\n",
      "Epoch: 096, Train Acc: 0.8602, Test Acc: 0.8225\n",
      "Epoch: 097, Train Acc: 0.8580, Test Acc: 0.8225\n",
      "Epoch: 098, Train Acc: 0.8691, Test Acc: 0.8136\n",
      "Epoch: 099, Train Acc: 0.8639, Test Acc: 0.8225\n",
      "Epoch: 100, Train Acc: 0.8654, Test Acc: 0.8136\n",
      "Epoch: 101, Train Acc: 0.8639, Test Acc: 0.8195\n",
      "Epoch: 102, Train Acc: 0.8639, Test Acc: 0.8195\n",
      "Epoch: 103, Train Acc: 0.8661, Test Acc: 0.8136\n",
      "Epoch: 104, Train Acc: 0.8602, Test Acc: 0.8225\n",
      "Epoch: 105, Train Acc: 0.8639, Test Acc: 0.8195\n",
      "Epoch: 106, Train Acc: 0.8632, Test Acc: 0.8195\n",
      "Epoch: 107, Train Acc: 0.8632, Test Acc: 0.8107\n",
      "Epoch: 108, Train Acc: 0.8669, Test Acc: 0.8195\n",
      "Epoch: 109, Train Acc: 0.8558, Test Acc: 0.7988\n",
      "Epoch: 110, Train Acc: 0.8595, Test Acc: 0.8195\n",
      "Epoch: 111, Train Acc: 0.8661, Test Acc: 0.8107\n",
      "Epoch: 112, Train Acc: 0.8609, Test Acc: 0.8225\n",
      "Epoch: 113, Train Acc: 0.8580, Test Acc: 0.8225\n",
      "Epoch: 114, Train Acc: 0.8624, Test Acc: 0.8195\n",
      "Epoch: 115, Train Acc: 0.8646, Test Acc: 0.8225\n",
      "Epoch: 116, Train Acc: 0.8602, Test Acc: 0.8254\n",
      "Epoch: 117, Train Acc: 0.8639, Test Acc: 0.8195\n",
      "Epoch: 118, Train Acc: 0.8669, Test Acc: 0.8225\n",
      "Epoch: 119, Train Acc: 0.8602, Test Acc: 0.8195\n",
      "Epoch: 120, Train Acc: 0.8661, Test Acc: 0.8254\n",
      "Epoch: 121, Train Acc: 0.8661, Test Acc: 0.8195\n",
      "Epoch: 122, Train Acc: 0.8646, Test Acc: 0.8166\n",
      "Epoch: 123, Train Acc: 0.8661, Test Acc: 0.8195\n",
      "Epoch: 124, Train Acc: 0.8654, Test Acc: 0.8254\n",
      "Epoch: 125, Train Acc: 0.8646, Test Acc: 0.8225\n",
      "Epoch: 126, Train Acc: 0.8587, Test Acc: 0.8343\n",
      "Epoch: 127, Train Acc: 0.8580, Test Acc: 0.8195\n",
      "Epoch: 128, Train Acc: 0.8706, Test Acc: 0.8462\n",
      "Epoch: 129, Train Acc: 0.8632, Test Acc: 0.8254\n",
      "Epoch: 130, Train Acc: 0.8698, Test Acc: 0.8254\n",
      "Epoch: 131, Train Acc: 0.8713, Test Acc: 0.8225\n",
      "Epoch: 132, Train Acc: 0.8676, Test Acc: 0.8225\n",
      "Epoch: 133, Train Acc: 0.8691, Test Acc: 0.8166\n",
      "Epoch: 134, Train Acc: 0.8661, Test Acc: 0.8225\n",
      "Epoch: 135, Train Acc: 0.8669, Test Acc: 0.8254\n",
      "Epoch: 136, Train Acc: 0.8609, Test Acc: 0.8195\n",
      "Epoch: 137, Train Acc: 0.8580, Test Acc: 0.8225\n",
      "Epoch: 138, Train Acc: 0.8728, Test Acc: 0.8195\n",
      "Epoch: 139, Train Acc: 0.8683, Test Acc: 0.8225\n",
      "Epoch: 140, Train Acc: 0.8743, Test Acc: 0.8077\n",
      "Epoch: 141, Train Acc: 0.8728, Test Acc: 0.8136\n",
      "Epoch: 142, Train Acc: 0.8713, Test Acc: 0.8373\n",
      "Epoch: 143, Train Acc: 0.8706, Test Acc: 0.8166\n",
      "Epoch: 144, Train Acc: 0.8728, Test Acc: 0.8314\n",
      "Epoch: 145, Train Acc: 0.8735, Test Acc: 0.8284\n",
      "Epoch: 146, Train Acc: 0.8735, Test Acc: 0.8254\n",
      "Epoch: 147, Train Acc: 0.8632, Test Acc: 0.8166\n",
      "Epoch: 148, Train Acc: 0.8720, Test Acc: 0.8195\n",
      "Epoch: 149, Train Acc: 0.8617, Test Acc: 0.8166\n",
      "Epoch: 150, Train Acc: 0.8698, Test Acc: 0.8225\n",
      "Epoch: 151, Train Acc: 0.8691, Test Acc: 0.8225\n",
      "Epoch: 152, Train Acc: 0.8617, Test Acc: 0.8225\n",
      "Epoch: 153, Train Acc: 0.8683, Test Acc: 0.8314\n",
      "Epoch: 154, Train Acc: 0.8624, Test Acc: 0.8225\n",
      "Epoch: 155, Train Acc: 0.8698, Test Acc: 0.8314\n",
      "Epoch: 156, Train Acc: 0.8669, Test Acc: 0.8195\n",
      "Epoch: 157, Train Acc: 0.8691, Test Acc: 0.8343\n",
      "Epoch: 158, Train Acc: 0.8728, Test Acc: 0.8254\n",
      "Epoch: 159, Train Acc: 0.8646, Test Acc: 0.8225\n",
      "Epoch: 160, Train Acc: 0.8706, Test Acc: 0.8343\n",
      "Epoch: 161, Train Acc: 0.8772, Test Acc: 0.8284\n",
      "Epoch: 162, Train Acc: 0.8757, Test Acc: 0.8284\n",
      "Epoch: 163, Train Acc: 0.8750, Test Acc: 0.8343\n",
      "Epoch: 164, Train Acc: 0.8676, Test Acc: 0.8284\n",
      "Epoch: 165, Train Acc: 0.8609, Test Acc: 0.8225\n",
      "Epoch: 166, Train Acc: 0.8624, Test Acc: 0.8225\n",
      "Epoch: 167, Train Acc: 0.8654, Test Acc: 0.8254\n",
      "Epoch: 168, Train Acc: 0.8683, Test Acc: 0.8373\n",
      "Epoch: 169, Train Acc: 0.8602, Test Acc: 0.8373\n",
      "Epoch: 170, Train Acc: 0.8728, Test Acc: 0.8343\n",
      "Epoch: 171, Train Acc: 0.8691, Test Acc: 0.8314\n",
      "Epoch: 172, Train Acc: 0.8765, Test Acc: 0.8314\n",
      "Epoch: 173, Train Acc: 0.8661, Test Acc: 0.8314\n",
      "Epoch: 174, Train Acc: 0.8691, Test Acc: 0.8225\n",
      "Epoch: 175, Train Acc: 0.8698, Test Acc: 0.8284\n",
      "Epoch: 176, Train Acc: 0.8691, Test Acc: 0.8314\n",
      "Epoch: 177, Train Acc: 0.8669, Test Acc: 0.8254\n",
      "Epoch: 178, Train Acc: 0.8617, Test Acc: 0.8254\n",
      "Epoch: 179, Train Acc: 0.8646, Test Acc: 0.8225\n",
      "Epoch: 180, Train Acc: 0.8587, Test Acc: 0.8225\n",
      "Epoch: 181, Train Acc: 0.8735, Test Acc: 0.8343\n",
      "Epoch: 182, Train Acc: 0.8661, Test Acc: 0.8284\n",
      "Epoch: 183, Train Acc: 0.8639, Test Acc: 0.8284\n",
      "Epoch: 184, Train Acc: 0.8750, Test Acc: 0.8314\n",
      "Epoch: 185, Train Acc: 0.8728, Test Acc: 0.8373\n",
      "Epoch: 186, Train Acc: 0.8728, Test Acc: 0.8314\n",
      "Epoch: 187, Train Acc: 0.8735, Test Acc: 0.8373\n",
      "Epoch: 188, Train Acc: 0.8787, Test Acc: 0.8343\n",
      "Epoch: 189, Train Acc: 0.8706, Test Acc: 0.8521\n",
      "Epoch: 190, Train Acc: 0.8706, Test Acc: 0.8373\n",
      "Epoch: 191, Train Acc: 0.8728, Test Acc: 0.8314\n",
      "Epoch: 192, Train Acc: 0.8743, Test Acc: 0.8284\n",
      "Epoch: 193, Train Acc: 0.8757, Test Acc: 0.8314\n",
      "Epoch: 194, Train Acc: 0.8720, Test Acc: 0.8402\n",
      "Epoch: 195, Train Acc: 0.8706, Test Acc: 0.8314\n",
      "Epoch: 196, Train Acc: 0.8706, Test Acc: 0.8225\n",
      "Epoch: 197, Train Acc: 0.8683, Test Acc: 0.8225\n",
      "Epoch: 198, Train Acc: 0.8743, Test Acc: 0.8402\n",
      "Epoch: 199, Train Acc: 0.8743, Test Acc: 0.8373\n",
      "Epoch: 200, Train Acc: 0.8728, Test Acc: 0.8284\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import Javascript\n",
    "display(Javascript('''google.colab.output.setIframeHeight(0, true, {maxHeight: 300})'''))\n",
    "\n",
    "model = GNN(hidden_channels=64)\n",
    "logger2 = MLLogger('gnn')\n",
    "print(model)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "for epoch in range(1, 201):\n",
    "    train()\n",
    "    train_acc = test(train_loader)\n",
    "    test_acc = test(test_loader)\n",
    "    print(f'Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}')\n",
    "    logger2.log_metrics(epoch=epoch,\n",
    "                        test_accuracy=test_acc,\n",
    "                        train_accuracy=train_acc,\n",
    "                        test_loss=None,\n",
    "                        train_loss=None)\n",
    "    \n",
    "# save logs\n",
    "log_name = str(datetime.datetime.now()).replace(\" \", \"_\").replace(\":\", \"_\").replace(\".\", \"_\")\n",
    "logger2.save_log(log_name + \".csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get particle which are wrongly classified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58\n",
      "(tensor(0), tensor(1), 'odo', '343503.vtp')\n",
      "(tensor(0), tensor(1), 'odo', '358687.vtp')\n",
      "(tensor(1), tensor(0), 'odo', '288510.vtp')\n",
      "(tensor(0), tensor(1), 'odo', '208345.vtp')\n",
      "(tensor(0), tensor(1), 'odo', '204808.vtp')\n",
      "(tensor(1), tensor(0), 'odo', '527677.vtp')\n",
      "(tensor(0), tensor(1), 'odo', '261809.vtp')\n",
      "(tensor(0), tensor(1), 'odo', '546335.vtp')\n",
      "(tensor(0), tensor(1), 'odo', '132346.vtp')\n",
      "(tensor(1), tensor(0), 'odo', '151718.vtp')\n",
      "(tensor(0), tensor(1), 'odo', '316046.vtp')\n",
      "(tensor(0), tensor(1), 'odo', '178677.vtp')\n",
      "(tensor(0), tensor(1), 'odo', '522249.vtp')\n",
      "(tensor(0), tensor(1), 'odo', '118026.vtp')\n",
      "(tensor(0), tensor(1), 'odo', '532689.vtp')\n",
      "(tensor(1), tensor(0), 'odo', '467331.vtp')\n",
      "(tensor(0), tensor(1), 'odo', '339686.vtp')\n",
      "(tensor(0), tensor(1), 'odo', '213507.vtp')\n",
      "(tensor(0), tensor(1), 'odo', '257888.vtp')\n",
      "(tensor(0), tensor(1), 'odo', '447626.vtp')\n",
      "(tensor(0), tensor(1), 'odo', '557818.vtp')\n",
      "(tensor(0), tensor(1), 'odo', '537408.vtp')\n",
      "(tensor(0), tensor(1), 'odo', '109321.vtp')\n",
      "(tensor(0), tensor(1), 'odo', '324690.vtp')\n",
      "(tensor(0), tensor(1), 'odo', '12472.vtp')\n",
      "(tensor(0), tensor(1), 'odo', '454663.vtp')\n",
      "(tensor(0), tensor(1), 'odo', '335699.vtp')\n",
      "(tensor(0), tensor(1), 'odo', '506176.vtp')\n",
      "(tensor(0), tensor(1), 'odo', '76458.vtp')\n",
      "(tensor(0), tensor(1), 'odo', '355490.vtp')\n",
      "(tensor(0), tensor(1), 'odo', '277813.vtp')\n",
      "(tensor(0), tensor(1), 'odo', '387090.vtp')\n",
      "(tensor(0), tensor(1), 'odo', '579073.vtp')\n",
      "(tensor(1), tensor(0), 'odo', '13795.vtp')\n",
      "(tensor(0), tensor(1), 'odo', '521264.vtp')\n",
      "(tensor(0), tensor(1), 'odo', '554393.vtp')\n",
      "(tensor(0), tensor(1), 'odo', '170000.vtp')\n",
      "(tensor(0), tensor(1), 'odo', '233577.vtp')\n",
      "(tensor(0), tensor(1), 'odo', '318808.vtp')\n",
      "(tensor(1), tensor(0), 'odo', '133466.vtp')\n",
      "(tensor(0), tensor(1), 'odo', '146591.vtp')\n",
      "(tensor(0), tensor(1), 'odo', '323670.vtp')\n",
      "(tensor(0), tensor(1), 'odo', '77090.vtp')\n",
      "(tensor(0), tensor(1), 'odo', '310980.vtp')\n",
      "(tensor(0), tensor(1), 'odo', '106808.vtp')\n",
      "(tensor(0), tensor(1), 'odo', '200667.vtp')\n",
      "(tensor(0), tensor(1), 'odo', '452435.vtp')\n",
      "(tensor(0), tensor(1), 'odo', '504471.vtp')\n",
      "(tensor(0), tensor(1), 'odo', '164652.vtp')\n",
      "(tensor(0), tensor(1), 'odo', '548866.vtp')\n",
      "(tensor(0), tensor(1), 'odo', '54441.vtp')\n",
      "(tensor(0), tensor(1), 'odo', '251901.vtp')\n",
      "(tensor(0), tensor(1), 'odo', '566236.vtp')\n",
      "(tensor(0), tensor(1), 'odo', '511127.vtp')\n",
      "(tensor(0), tensor(1), 'odo', '62053.vtp')\n",
      "(tensor(0), tensor(1), 'odo', '242797.vtp')\n",
      "(tensor(1), tensor(0), 'odo', '546663.vtp')\n",
      "(tensor(0), tensor(1), 'odo', '414231.vtp')\n"
     ]
    }
   ],
   "source": [
    "wrong_pred = []\n",
    "for data in test_loader:\n",
    "    out = model(data.x, data.edge_index, data.batch)\n",
    "    pred = out.argmax(dim=1)\n",
    "    for i in range(len(pred)):\n",
    "        if pred[i] != data.y[i]:\n",
    "            wrong_pred.append((data.y[i], pred[i], data.dataset_name[i], data.file_name[i]))\n",
    "print(len(wrong_pred))\n",
    "for wp in wrong_pred:\n",
    "    print(wp)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# visualize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
